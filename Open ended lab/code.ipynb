{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.1+6a7e0ae)\n",
      "[Powered by Stella]\n",
      "Game console created:\n",
      "  ROM file:  /opt/anaconda3/lib/python3.12/site-packages/ale_py/roms/breakout.bin\n",
      "  Cart Name: Breakout - Breakaway IV (1978) (Atari)\n",
      "  Cart MD5:  f34f08e5eb96e500e851a80be3277a56\n",
      "  Display Format:  AUTO-DETECT ==> NTSC\n",
      "  ROM Size:        2048\n",
      "  Bankswitch Type: AUTO-DETECT ==> 2K\n",
      "\n",
      "Running ROM file...\n",
      "Random seed is 1734803125\n"
     ]
    }
   ],
   "source": [
    "from ale_py import ALEInterface, roms\n",
    "\n",
    "ale = ALEInterface()\n",
    "ale.loadROM(roms.get_rom_path(\"breakout\"))\n",
    "ale.reset_game()\n",
    "\n",
    "reward = ale.act(0)  # noop\n",
    "screen_obs = ale.getScreenRGB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "\n",
    "gym.register_envs(ale_py)  # unnecessary but helpful for IDEs\n",
    "\n",
    "env = gym.make('ALE/Breakout-v5', render_mode=\"human\")  # remove render_mode in training\n",
    "obs, info = env.reset()\n",
    "episode_over = False\n",
    "# while not episode_over:\n",
    "#     action = policy(obs)  # to implement - use `env.action_space.sample()` for a random policy\n",
    "#     obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "#     episode_over = terminated or truncated\n",
    "# env.close()\n",
    "\n",
    "# print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  20%|██        | 100/500 [1:22:59<5:46:10, 51.93s/episode, Total Reward=2, Epsilon=0.4915, Avg Loss=0.0144]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Total Reward: 2.0, Epsilon: 0.4915, Average Loss: 0.0144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  40%|████      | 200/500 [2:32:59<3:14:59, 39.00s/episode, Total Reward=1, Epsilon=0.4830, Avg Loss=0.0081]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200, Total Reward: 1.0, Epsilon: 0.4830, Average Loss: 0.0081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  60%|██████    | 300/500 [3:57:02<3:04:00, 55.20s/episode, Total Reward=4, Epsilon=0.4727, Avg Loss=0.0089]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300, Total Reward: 4.0, Epsilon: 0.4727, Average Loss: 0.0089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  80%|███████▉  | 398/500 [5:44:08<1:27:20, 51.38s/episode, Total Reward=3, Epsilon=0.4606, Avg Loss=0.0200] "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt  # For plotting graphs\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Register Atari environments (optional, helps IDEs)\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "# Hyperparameters\n",
    "ENV_NAME = 'ALE/Breakout-v5'\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "MEMORY_SIZE = 10000\n",
    "MIN_MEMORY_SIZE = 1000\n",
    "EPS_START = 0.5\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 1000000\n",
    "TARGET_UPDATE_FREQ = 100\n",
    "NUM_EPISODES = 500\n",
    "MAX_STEPS = 1000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Preprocesses a single frame: grayscale, resize, normalize.\"\"\"\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n",
    "    frame = cv2.resize(frame, (84, 84), interpolation=cv2.INTER_AREA)  # Resize to 84x84\n",
    "    frame = frame / 255.0  # Normalize pixel values\n",
    "    return frame\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"Simplified Deep Q-Network for faster training.\"\"\"\n",
    "    def __init__(self, input_channels, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=8, stride=4),  # Reduced channels\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),  # Reduced channels\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1),  # Reduced channels\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 7 * 7, 256),  # Reduced linear layer size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience Replay Buffer.\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.tensor(states, dtype=torch.float32).to(DEVICE),\n",
    "            torch.tensor(actions, dtype=torch.long).to(DEVICE),\n",
    "            torch.tensor(rewards, dtype=torch.float32).to(DEVICE),\n",
    "            torch.tensor(next_states, dtype=torch.float32).to(DEVICE),\n",
    "            torch.tensor(dones, dtype=torch.float32).to(DEVICE)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def select_action(state, policy_net, epsilon, num_actions):\n",
    "    \"\"\"Selects an action using epsilon-greedy policy.\"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(num_actions)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "            q_values = policy_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "def plot_metrics(episodes, rewards, epsilons, losses, save_dir, episode):\n",
    "    \"\"\"Plots and saves the training metrics after each episode.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Plot Total Rewards per Episode\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(episodes, rewards, label='Total Reward per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Total Rewards Over Episodes')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, f'total_rewards_episode.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot Epsilon Decay\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(episodes, epsilons, label='Epsilon Value')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon')\n",
    "    plt.title('Epsilon Decay Over Episodes')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, f'epsilon_decay_episode.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot Loss Over Time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(episodes, losses, label='Loss')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Over Episodes')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, f'loss_episode.png'))\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    # Initialize environment\n",
    "    env = gym.make(ENV_NAME)\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # Initialize networks\n",
    "    input_channels = 4  # Stacked frames\n",
    "    policy_net = DQN(input_channels, num_actions).to(DEVICE)\n",
    "    target_net = DQN(input_channels, num_actions).to(DEVICE)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Initialize replay buffer\n",
    "    replay_buffer = ReplayBuffer(MEMORY_SIZE)\n",
    "\n",
    "    epsilon = EPS_START\n",
    "    epsilon_decay = (EPS_START - EPS_END) / EPS_DECAY\n",
    "\n",
    "    # Lists to store metrics\n",
    "    episode_rewards = []\n",
    "    episode_epsilons = []\n",
    "    episode_losses = []\n",
    "    episodes = []\n",
    "\n",
    "    # Directory to save plots and model\n",
    "    save_directory = 'dqn_training_results'\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "    episode_bar = tqdm(range(1, NUM_EPISODES + 1), desc=\"Training Episodes\", unit=\"episode\")\n",
    "    for episode in episode_bar:\n",
    "        obs, info = env.reset()\n",
    "        state = preprocess_frame(obs)\n",
    "        state_stack = deque([state] * 4, maxlen=4)  # Initialize with 4 frames\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        loss_per_episode = 0\n",
    "        steps = 0\n",
    "\n",
    "        for step in range(MAX_STEPS):\n",
    "            stacked_state = np.stack(state_stack, axis=0)\n",
    "            action = select_action(stacked_state, policy_net, epsilon, num_actions)\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "\n",
    "            next_state = preprocess_frame(next_obs)\n",
    "            state_stack.append(next_state)\n",
    "            stacked_next_state = np.stack(state_stack, axis=0)\n",
    "\n",
    "            replay_buffer.push(stacked_state, action, reward, stacked_next_state, done)\n",
    "\n",
    "            if len(replay_buffer) > MIN_MEMORY_SIZE:\n",
    "                # Sample a batch\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "                # Compute current Q values\n",
    "                q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                # Compute target Q values\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = target_net(next_states).max(1)[0]\n",
    "                    target_q_values = rewards + GAMMA * next_q_values * (1 - dones)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(q_values, target_q_values)\n",
    "                loss_per_episode += loss.item()\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update epsilon\n",
    "                if epsilon > EPS_END:\n",
    "                    epsilon -= epsilon_decay\n",
    "\n",
    "                # Update target network\n",
    "                if step % TARGET_UPDATE_FREQ == 0:\n",
    "                    target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            steps += 1\n",
    "\n",
    "        # Record metrics\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_epsilons.append(epsilon)\n",
    "        average_loss = loss_per_episode / steps if steps > 0 else 0\n",
    "        episode_losses.append(average_loss)\n",
    "        episodes.append(episode)\n",
    "\n",
    "        # Update progress bar with latest metrics\n",
    "        episode_bar.set_postfix({\n",
    "            'Total Reward': total_reward,\n",
    "            'Epsilon': f\"{epsilon:.4f}\",\n",
    "            'Avg Loss': f\"{average_loss:.4f}\"\n",
    "        })\n",
    "\n",
    "        # Plot and save metrics after each episode\n",
    "        plot_metrics(episodes, episode_rewards, episode_epsilons, episode_losses, save_directory, episode)\n",
    "\n",
    "        # Print progress every 100 episodes\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.4f}, Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "    # Plot and save final metrics\n",
    "    plot_metrics(episodes, episode_rewards, episode_epsilons, episode_losses, save_directory, 'final')\n",
    "\n",
    "    # Save the trained model\n",
    "    model_path = os.path.join(save_directory, 'dqn_breakout_model.pth')\n",
    "    torch.save(policy_net.state_dict(), model_path)\n",
    "    print(f\"Training completed! Model saved at {model_path}\")\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
